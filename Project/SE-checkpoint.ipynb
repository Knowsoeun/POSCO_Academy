{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('./aibd-16'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Do not edit this shell ==========================================\n",
    "\n",
    "# Dataset Definition\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None):\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            root = root + 'train.csv'\n",
    "        else:\n",
    "            root = root + 'test.csv'\n",
    "        self.csv = pd.read_csv(root, header=None)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            label = torch.tensor(self.csv.iloc[index,0], dtype=torch.long)\n",
    "            img = np.array(self.csv.iloc[index,1:]/255).reshape(28, 28)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        else:\n",
    "            img = np.array(self.csv.loc[index]/255).reshape(28, 28)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "\n",
    "# ============================== Do not edit this shell =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Importation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "# Hyper Parameter\n",
    "## Data Loader\n",
    "batch_size = 8\n",
    "\n",
    "## Model\n",
    "hidden_layer = 512\n",
    "\n",
    "## Learning\n",
    "logging_dispfig = True\n",
    "maximum_epoch = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Device Preparation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{\"CPU\" if device == \"cpu\" else \"GPU\"} will be used in training/validation.')\n",
    "\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "# root = '/kaggle/input/aibd-16/'\n",
    "root = './'\n",
    "train_data = CharDataset(root, train=True, transform=ToTensor())\n",
    "train_data, valid_data = random_split(train_data, [round(len(train_data)*0.9), round(len(train_data)*0.1)])\n",
    "test_data = CharDataset(root, train=False, transform=ToTensor())\n",
    "\n",
    "# Check the data\n",
    "print('===================== Check the data =========================\\n')\n",
    "print(f'Train dataset length = {len(train_data)}')\n",
    "print(f'Valid dataset length = {len(valid_data)}')\n",
    "print(f'Test dataset length = {len(test_data)}\\n')\n",
    "\n",
    "train_0_x, train_0_y = train_data[0]\n",
    "print(f'Content of Y (Label, type={type(train_0_y)}) = {train_0_y}')\n",
    "print(f'Shape of X (Data, type={type(train_0_x)}) = {train_0_x.shape}')\n",
    "plt.figure(1)\n",
    "plt.imshow(train_0_x.squeeze())\n",
    "plt.title(f'train_0_x ({train_0_x.squeeze().shape})')\n",
    "plt.show()\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                          drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=len(valid_data), pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), pin_memory=True)\n",
    "\n",
    "# Examine the data loader\n",
    "print('================== Check the data loader ======================\\n')\n",
    "train_enumerator = enumerate(train_loader)\n",
    "ex_batch_idx, (ex_data, ex_label) = next(train_enumerator)\n",
    "print(f'Idx: {ex_batch_idx} / X.shape = {ex_data.shape} / Conv2dY.shape = {ex_label.shape}\\n')\n",
    "print(f'Y[0:{batch_size}] = {ex_label}')\n",
    "preview_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_0_x.view([-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### soeun_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class soeun_model(nn.Module):\n",
    "    \n",
    "    ### Put Your Script Here ###\n",
    "    def __init__(self,num_classes=1000):\n",
    "        super(soeun_model, self).__init__()\n",
    "\n",
    "        \n",
    "        self.layer1=nn.Sequential(                             #nn.Sequential : 코드에 적힌 순서대로 값을 전달해 처리\n",
    "        ## [Layer 1] Convolution \n",
    "        nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  #input_channel_size = 1, output_volume_size = 64\n",
    "        nn.BatchNorm2d(64), nn.LeakyReLU(0.1),\n",
    "    \n",
    "        ## [Layer 2] Max Pooling \n",
    "        nn.MaxPool2d(2),\n",
    "            \n",
    "        ## [Layer 3] Convolution\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.LeakyReLU(0.1),\n",
    "        ## [Layer 4] Max Pooling  \n",
    "        nn.MaxPool2d(2),\n",
    "              \n",
    "        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(256), nn.LeakyReLU(0.1),\n",
    "        nn.MaxPool2d(2),\n",
    "        \n",
    "        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512), nn.LeakyReLU(0.1),\n",
    "        nn.MaxPool2d(2)\n",
    "            \n",
    "        )\n",
    "       \n",
    "\n",
    "    \n",
    "#         self.channelavg_part=nn.AvgPool2d(3)\n",
    "#         self.fc=nn.Linear(1*1*256, 10) # 1*1은 이미지 차원에 해당\n",
    "        self.fc=nn.Linear(1*1*512, 10)\n",
    "        # weight 초기화\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "    def forward(self, x): #method가 존재해야 한다. 해당 method 리턴값이 output이 된다는 의미\n",
    "        out=self.layer1(x)\n",
    "#       out=self.channelavg_part(out)\n",
    "        out=out.reshape(out.size(0), -1)\n",
    "        onehot_out=self.fc(out)\n",
    "        return onehot_out       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "  \n",
    "# Model\n",
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    net = soeun_model().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()                   # 손실함수는 CrossEntropy 사용\n",
    "    optim = Adam(net.parameters(), lr=learning_rate)  # 옵티마이저는 Adam을 사용\n",
    "\n",
    "# Epoch\n",
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0  #epoch 수 초기화\n",
    "\n",
    "    \n",
    "def epoch(data_loader):\n",
    "    # One epoch : gets data_loader as input and returns loss / accuracy, and\n",
    "    #             last prediction value / its label(truth) value for future use\n",
    "    global epoch_cnt\n",
    "    iter_loss, iter_acc = [], []\n",
    "\n",
    "    last_grad_performed = False\n",
    "\n",
    "    # Mini-batch iterations\n",
    "    for _data, _label in data_loader:\n",
    "        data, label = _data.to(device), _label.to(device)\n",
    "      #  print(\"In Epoch problem1: \", data.shape)\n",
    "     \n",
    "        # 1. Feed-forward\n",
    "        onehot_out = net(data)\n",
    "      #  print(\"In Epoch problem2\")\n",
    "        # 2. Calculate accuracy\n",
    "        _, out = torch.max(onehot_out, 1)\n",
    "        acc_partial = (out == label).float().sum()\n",
    "        acc_partial = acc_partial / len(label)\n",
    "        iter_acc.append(acc_partial.item())\n",
    "\n",
    "        # 3. Calculate loss\n",
    "        loss = loss_fn(onehot_out, label)\n",
    "        iter_loss.append(loss.item())\n",
    "\n",
    "        # 4. Backward propagation if not in `torch.no_grad()`\n",
    "        if onehot_out.requires_grad:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            last_grad_performed = True\n",
    "\n",
    "    # Up epoch count if backward propagation is done\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    # Clear memory to prevent CUDA memory error\n",
    "    clear_memory()\n",
    "\n",
    "    return np.average(iter_loss), np.average(iter_acc)\n",
    "\n",
    "\n",
    "def epoch_not_finished():\n",
    "    # For now, let's repeat training fixed times, e.g. 25 times.\n",
    "    # We will learn how to determine training stop or continue later.\n",
    "    return epoch_cnt < maximum_epoch # 설정한 epoch 최댓값보다 epoch 수가 적으면 끝내지 않음\n",
    "\n",
    "# Logging\n",
    "def init_log():\n",
    "    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    time_log, log_stack = [], []\n",
    "  \n",
    "  \n",
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    # Push time, training loss, training accuracy, and epoch count into lists\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)\n",
    "  \n",
    "  \n",
    "def record_valid_log(_vloss, _vacc):\n",
    "    # Push validation loss and validation accuracy into each list\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)\n",
    "\n",
    "    \n",
    "def last(log_list):\n",
    "    # Get the last member of list. If empty, return -1.\n",
    "    if len(log_list) > 0: return log_list[len(log_list) - 1]\n",
    "    else: return -1\n",
    "\n",
    "\n",
    "def print_log():\n",
    "    # Generate log string and put it into log stack\n",
    "    log_str = f'Iter: {last(iter_log):>4d} >> T_loss {last(tloss_log):<8.5f}   ' \\\n",
    "          + f'T_acc {last(tacc_log):<6.5f}   V_loss {last(vloss_log):<8.5f}   ' \\\n",
    "          + f'V_acc {last(vacc_log):<6.5f}   🕒 {last(time_log):5.3f}s'\n",
    "    log_stack.append(log_str)\n",
    "  \n",
    "  # Draw figure if want\n",
    "    if logging_dispfig:\n",
    "        hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n",
    "        hist_fig.patch.set_facecolor('white')\n",
    "\n",
    "        # Draw loss lines\n",
    "        loss_t_line = plt.plot(iter_log, tloss_log, label='Train Loss', color='#FF9999', marker='o')\n",
    "        loss_v_line = plt.plot(iter_log, vloss_log, label='Valid Loss', color='#99B0FF', marker='s')\n",
    "        loss_axis.set_xlabel('epoch')\n",
    "        loss_axis.set_ylabel('loss')\n",
    "\n",
    "        # Draw accuracy lines\n",
    "        acc_axis = loss_axis.twinx()\n",
    "        acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train Acc.', color='#FF0000', marker='+')\n",
    "        acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid Acc.', color='#003AFF', marker='x')\n",
    "        acc_axis.set_ylabel('accuracy')\n",
    "\n",
    "        # Append annotations\n",
    "        hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n",
    "        loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n",
    "        loss_axis.grid()\n",
    "        plt.title(f'Learning history until epoch {last(iter_log)}')\n",
    "        plt.draw()\n",
    "    \n",
    "  # Print log\n",
    "    clear_output(wait=True)\n",
    "    if logging_dispfig: plt.show()\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import memory_allocated, empty_cache\n",
    "import gc\n",
    "# Memory cleaner to prevent CUDA out of memory error\n",
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpu 캐시 비우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Initialization\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "\n",
    "# Training Iteration\n",
    "while epoch_not_finished():\n",
    "    net.train()\n",
    "    start_time = time.time()\n",
    "    tloss, tacc = epoch(train_loader)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        vloss, vacc= epoch(valid_loader)\n",
    "        record_valid_log(vloss, vacc)\n",
    "    print_log()\n",
    "    \n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prediction vector to CSV file\n",
    "\n",
    "# Before run this code, here put your save path (only for local server not kaggle kernel)\n",
    "save_root = './test_pred.csv'   # <--- only edit this path\n",
    "\n",
    "# After run this code, you must check that the shape of 'out' variable is 3745. (out.shape == 3745)\n",
    "\n",
    "\n",
    "# ============================== Do not edit under this line ==========================================\n",
    "\n",
    "for _data in test_loader:\n",
    "    print(\"Test Loader: 1\")\n",
    "    print(\"Test Loader: {}\".format(_data.shape))\n",
    "    data= _data.to(device)\n",
    "    #data = _data.view([len(_data), -1]).to(device)\n",
    "\n",
    "    # 1. Feed-forward\n",
    "    onehot_out = net(data)\n",
    "    _, out = torch.max(onehot_out, 1)\n",
    "    \n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "# 덮어쓰기 방지를 위해 이미 파일이 존재하면 삭제\n",
    "if os.path.isfile(save_root):\n",
    "    os.remove(save_root)\n",
    "\n",
    "# 첫 행에 'id' 'lable' 그 다음 행부터 idx와 label 넣어서 csv 저장\n",
    "for idx, pred in enumerate(list(out.cpu())):\n",
    "    with open(save_root, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if idx == 0:\n",
    "            writer.writerow(['Id', 'Category'])\n",
    "        pred = np.concatenate(([idx], [pred]))\n",
    "        writer.writerow(pred)\n",
    "# ============================== Do not edit over this line =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
