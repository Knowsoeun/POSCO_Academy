{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('./aibd-16'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Do not edit this shell ==========================================\n",
    "\n",
    "# Dataset Definition\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None):\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            root = root + 'train.csv'\n",
    "        else:\n",
    "            root = root + 'test.csv'\n",
    "        self.csv = pd.read_csv(root, header=None)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            label = torch.tensor(self.csv.iloc[index,0], dtype=torch.long)\n",
    "            img = np.array(self.csv.iloc[index,1:]/255).reshape(28, 28)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        else:\n",
    "            img = np.array(self.csv.loc[index]/255).reshape(28, 28)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "\n",
    "# ============================== Do not edit this shell =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Importation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "# Hyper Parameter\n",
    "## Data Loader\n",
    "batch_size = 8\n",
    "\n",
    "## Model\n",
    "hidden_layer = 512\n",
    "\n",
    "## Learning\n",
    "logging_dispfig = True\n",
    "maximum_epoch = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Device Preparation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{\"CPU\" if device == \"cpu\" else \"GPU\"} will be used in training/validation.')\n",
    "\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "# root = '/kaggle/input/aibd-16/'\n",
    "root = './'\n",
    "train_data = CharDataset(root, train=True, transform=ToTensor())\n",
    "train_data, valid_data = random_split(train_data, [round(len(train_data)*0.9), round(len(train_data)*0.1)])\n",
    "test_data = CharDataset(root, train=False, transform=ToTensor())\n",
    "\n",
    "# Check the data\n",
    "print('===================== Check the data =========================\\n')\n",
    "print(f'Train dataset length = {len(train_data)}')\n",
    "print(f'Valid dataset length = {len(valid_data)}')\n",
    "print(f'Test dataset length = {len(test_data)}\\n')\n",
    "\n",
    "train_0_x, train_0_y = train_data[0]\n",
    "print(f'Content of Y (Label, type={type(train_0_y)}) = {train_0_y}')\n",
    "print(f'Shape of X (Data, type={type(train_0_x)}) = {train_0_x.shape}')\n",
    "plt.figure(1)\n",
    "plt.imshow(train_0_x.squeeze())\n",
    "plt.title(f'train_0_x ({train_0_x.squeeze().shape})')\n",
    "plt.show()\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                          drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=len(valid_data), pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), pin_memory=True)\n",
    "\n",
    "# Examine the data loader\n",
    "print('================== Check the data loader ======================\\n')\n",
    "train_enumerator = enumerate(train_loader)\n",
    "ex_batch_idx, (ex_data, ex_label) = next(train_enumerator)\n",
    "print(f'Idx: {ex_batch_idx} / X.shape = {ex_data.shape} / Conv2dY.shape = {ex_label.shape}\\n')\n",
    "print(f'Y[0:{batch_size}] = {ex_label}')\n",
    "preview_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_0_x.view([-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### soeun_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class soeun_model(nn.Module):\n",
    "    \n",
    "    ### Put Your Script Here ###\n",
    "    def __init__(self,num_classes=1000):\n",
    "        super(soeun_model, self).__init__()\n",
    "\n",
    "        \n",
    "        self.layer1=nn.Sequential(                             #nn.Sequential : ÏΩîÎìúÏóê Ï†ÅÌûå ÏàúÏÑúÎåÄÎ°ú Í∞íÏùÑ Ï†ÑÎã¨Ìï¥ Ï≤òÎ¶¨\n",
    "        ## [Layer 1] Convolution \n",
    "        nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  #input_channel_size = 1, output_volume_size = 64\n",
    "        nn.BatchNorm2d(64), nn.LeakyReLU(0.1),\n",
    "    \n",
    "        ## [Layer 2] Max Pooling \n",
    "        nn.MaxPool2d(2),\n",
    "            \n",
    "        ## [Layer 3] Convolution\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.LeakyReLU(0.1),\n",
    "        ## [Layer 4] Max Pooling  \n",
    "        nn.MaxPool2d(2),\n",
    "              \n",
    "        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(256), nn.LeakyReLU(0.1),\n",
    "        nn.MaxPool2d(2),\n",
    "        \n",
    "        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512), nn.LeakyReLU(0.1),\n",
    "        nn.MaxPool2d(2)\n",
    "            \n",
    "        )\n",
    "       \n",
    "\n",
    "    \n",
    "#         self.channelavg_part=nn.AvgPool2d(3)\n",
    "#         self.fc=nn.Linear(1*1*256, 10) # 1*1ÏùÄ Ïù¥ÎØ∏ÏßÄ Ï∞®ÏõêÏóê Ìï¥Îãπ\n",
    "        self.fc=nn.Linear(1*1*512, 10)\n",
    "        # weight Ï¥àÍ∏∞Ìôî\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "    def forward(self, x): #methodÍ∞Ä Ï°¥Ïû¨Ìï¥Ïïº ÌïúÎã§. Ìï¥Îãπ method Î¶¨ÌÑ¥Í∞íÏù¥ outputÏù¥ ÎêúÎã§Îäî ÏùòÎØ∏\n",
    "        out=self.layer1(x)\n",
    "#       out=self.channelavg_part(out)\n",
    "        out=out.reshape(out.size(0), -1)\n",
    "        onehot_out=self.fc(out)\n",
    "        return onehot_out       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "  \n",
    "# Model\n",
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    net = soeun_model().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()                   # ÏÜêÏã§Ìï®ÏàòÎäî CrossEntropy ÏÇ¨Ïö©\n",
    "    optim = Adam(net.parameters(), lr=learning_rate)  # ÏòµÌã∞ÎßàÏù¥Ï†ÄÎäî AdamÏùÑ ÏÇ¨Ïö©\n",
    "\n",
    "# Epoch\n",
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0  #epoch Ïàò Ï¥àÍ∏∞Ìôî\n",
    "\n",
    "    \n",
    "def epoch(data_loader):\n",
    "    # One epoch : gets data_loader as input and returns loss / accuracy, and\n",
    "    #             last prediction value / its label(truth) value for future use\n",
    "    global epoch_cnt\n",
    "    iter_loss, iter_acc = [], []\n",
    "\n",
    "    last_grad_performed = False\n",
    "\n",
    "    # Mini-batch iterations\n",
    "    for _data, _label in data_loader:\n",
    "        data, label = _data.to(device), _label.to(device)\n",
    "      #  print(\"In Epoch problem1: \", data.shape)\n",
    "     \n",
    "        # 1. Feed-forward\n",
    "        onehot_out = net(data)\n",
    "      #  print(\"In Epoch problem2\")\n",
    "        # 2. Calculate accuracy\n",
    "        _, out = torch.max(onehot_out, 1)\n",
    "        acc_partial = (out == label).float().sum()\n",
    "        acc_partial = acc_partial / len(label)\n",
    "        iter_acc.append(acc_partial.item())\n",
    "\n",
    "        # 3. Calculate loss\n",
    "        loss = loss_fn(onehot_out, label)\n",
    "        iter_loss.append(loss.item())\n",
    "\n",
    "        # 4. Backward propagation if not in `torch.no_grad()`\n",
    "        if onehot_out.requires_grad:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            last_grad_performed = True\n",
    "\n",
    "    # Up epoch count if backward propagation is done\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    # Clear memory to prevent CUDA memory error\n",
    "    clear_memory()\n",
    "\n",
    "    return np.average(iter_loss), np.average(iter_acc)\n",
    "\n",
    "\n",
    "def epoch_not_finished():\n",
    "    # For now, let's repeat training fixed times, e.g. 25 times.\n",
    "    # We will learn how to determine training stop or continue later.\n",
    "    return epoch_cnt < maximum_epoch # ÏÑ§Ï†ïÌïú epoch ÏµúÎåìÍ∞íÎ≥¥Îã§ epoch ÏàòÍ∞Ä Ï†ÅÏúºÎ©¥ ÎÅùÎÇ¥ÏßÄ ÏïäÏùå\n",
    "\n",
    "# Logging\n",
    "def init_log():\n",
    "    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    time_log, log_stack = [], []\n",
    "  \n",
    "  \n",
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    # Push time, training loss, training accuracy, and epoch count into lists\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)\n",
    "  \n",
    "  \n",
    "def record_valid_log(_vloss, _vacc):\n",
    "    # Push validation loss and validation accuracy into each list\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)\n",
    "\n",
    "    \n",
    "def last(log_list):\n",
    "    # Get the last member of list. If empty, return -1.\n",
    "    if len(log_list) > 0: return log_list[len(log_list) - 1]\n",
    "    else: return -1\n",
    "\n",
    "\n",
    "def print_log():\n",
    "    # Generate log string and put it into log stack\n",
    "    log_str = f'Iter: {last(iter_log):>4d} >> T_loss {last(tloss_log):<8.5f}   ' \\\n",
    "          + f'T_acc {last(tacc_log):<6.5f}   V_loss {last(vloss_log):<8.5f}   ' \\\n",
    "          + f'V_acc {last(vacc_log):<6.5f}   üïí {last(time_log):5.3f}s'\n",
    "    log_stack.append(log_str)\n",
    "  \n",
    "  # Draw figure if want\n",
    "    if logging_dispfig:\n",
    "        hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n",
    "        hist_fig.patch.set_facecolor('white')\n",
    "\n",
    "        # Draw loss lines\n",
    "        loss_t_line = plt.plot(iter_log, tloss_log, label='Train Loss', color='#FF9999', marker='o')\n",
    "        loss_v_line = plt.plot(iter_log, vloss_log, label='Valid Loss', color='#99B0FF', marker='s')\n",
    "        loss_axis.set_xlabel('epoch')\n",
    "        loss_axis.set_ylabel('loss')\n",
    "\n",
    "        # Draw accuracy lines\n",
    "        acc_axis = loss_axis.twinx()\n",
    "        acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train Acc.', color='#FF0000', marker='+')\n",
    "        acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid Acc.', color='#003AFF', marker='x')\n",
    "        acc_axis.set_ylabel('accuracy')\n",
    "\n",
    "        # Append annotations\n",
    "        hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n",
    "        loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n",
    "        loss_axis.grid()\n",
    "        plt.title(f'Learning history until epoch {last(iter_log)}')\n",
    "        plt.draw()\n",
    "    \n",
    "  # Print log\n",
    "    clear_output(wait=True)\n",
    "    if logging_dispfig: plt.show()\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import memory_allocated, empty_cache\n",
    "import gc\n",
    "# Memory cleaner to prevent CUDA out of memory error\n",
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpu Ï∫êÏãú ÎπÑÏö∞Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ïã§Ìñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Initialization\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "\n",
    "# Training Iteration\n",
    "while epoch_not_finished():\n",
    "    net.train()\n",
    "    start_time = time.time()\n",
    "    tloss, tacc = epoch(train_loader)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        vloss, vacc= epoch(valid_loader)\n",
    "        record_valid_log(vloss, vacc)\n",
    "    print_log()\n",
    "    \n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prediction vector to CSV file\n",
    "\n",
    "# Before run this code, here put your save path (only for local server not kaggle kernel)\n",
    "save_root = './test_pred.csv'   # <--- only edit this path\n",
    "\n",
    "# After run this code, you must check that the shape of 'out' variable is 3745. (out.shape == 3745)\n",
    "\n",
    "\n",
    "# ============================== Do not edit under this line ==========================================\n",
    "\n",
    "for _data in test_loader:\n",
    "    print(\"Test Loader: 1\")\n",
    "    print(\"Test Loader: {}\".format(_data.shape))\n",
    "    data= _data.to(device)\n",
    "    #data = _data.view([len(_data), -1]).to(device)\n",
    "\n",
    "    # 1. Feed-forward\n",
    "    onehot_out = net(data)\n",
    "    _, out = torch.max(onehot_out, 1)\n",
    "    \n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "# ÎçÆÏñ¥Ïì∞Í∏∞ Î∞©ÏßÄÎ•º ÏúÑÌï¥ Ïù¥ÎØ∏ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÎ©¥ ÏÇ≠Ï†ú\n",
    "if os.path.isfile(save_root):\n",
    "    os.remove(save_root)\n",
    "\n",
    "# Ï≤´ ÌñâÏóê 'id' 'lable' Í∑∏ Îã§Ïùå ÌñâÎ∂ÄÌÑ∞ idxÏôÄ label ÎÑ£Ïñ¥ÏÑú csv Ï†ÄÏû•\n",
    "for idx, pred in enumerate(list(out.cpu())):\n",
    "    with open(save_root, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if idx == 0:\n",
    "            writer.writerow(['Id', 'Category'])\n",
    "        pred = np.concatenate(([idx], [pred]))\n",
    "        writer.writerow(pred)\n",
    "# ============================== Do not edit over this line =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
